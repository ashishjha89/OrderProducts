# Phase-1: Deploy to single EC2 instance

## Executive Summary

OrderProducts system requires **ZERO code changes** to deploy to AWS EC2!

The system is already perfectly architected for single-instance cloud deployment. You can deploy the exact same docker-compose setup that runs locally to AWS EC2 without any modifications to application code, Dockerfiles, or docker-compose.yml.

---

## Strategic Decisions & Rationale

### 1. Configuration Strategy: Environment Variables (Option A)

**Decision**: Use environment variables exclusively, avoiding Spring profiles or separate docker-compose files.

**Why This Option:**
- âœ… **Cloud-Native**: Aligns with 12-Factor App principles, making future migration to ECS Fargate trivial
- âœ… **Terraform-Friendly**: Environment variables map directly to Terraform resource definitions
- âœ… **Zero Config Drift**: Single source of truth - no risk of maintaining divergent config files
- âœ… **Container Orchestration Ready**: Works seamlessly with Docker Compose, ECS, Kubernetes
- âœ… **Local-AWS Parity**: Same codebase and Docker images run in both environments

**Current State**: âœ… **Already implemented in your codebase**
- All services use `${VAR:default}` pattern
- All configurations externalized via environment variables
- docker-compose.yml already provides all necessary env vars

---

### 2. Port Exposure: Security vs Convenience

Should 8761 (Eureka), 9411 (Zipkin) be exposed publicly?

1. **Eureka exposes service topology** - attackers can see your internal architecture
2. **Zipkin contains request/response traces** - may leak sensitive data patterns
3. **Both have no authentication** - anyone with access can view everything

For a practice project running few hours/month, you have three options:

---

#### ğŸ¯ Three Security Approaches:

**Option 1: Minimal Exposure + SSH Tunneling (Most Secure)** â­ **RECOMMENDED**

**Security Group Configuration:**
```
Inbound Rules:
â”Œâ”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Type â”‚ Port â”‚   Source    â”‚     Description     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ SSH  â”‚  22  â”‚ Your IP/32  â”‚ SSH access          â”‚
â”‚ HTTP â”‚ 8080 â”‚ Your IP/32  â”‚ API Gateway         â”‚
â””â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Ports 8761 (Eureka) and 9411 (Zipkin): NOT exposed
```

**Access Monitoring Tools:**
```bash
# Open SSH tunnel from your local machine
ssh -i ~/.ssh/your-key.pem \
  -L 8761:localhost:8761 \
  -L 9411:localhost:9411 \
  ec2-user@<EC2_IP>

# Then access in browser via localhost
http://localhost:8761  # Eureka dashboard
http://localhost:9411  # Zipkin UI
```

**Pros:**
- âœ… Zero attack surface for monitoring tools
- âœ… No additional ports exposed to internet
- âœ… Even if someone finds your EC2 IP, they can't access Eureka/Zipkin
- âœ… Best practice for production-like environments

**Cons:**
- âš ï¸ Requires SSH connection open while debugging
- âš ï¸ Slightly less convenient (but worth the security)

---

**Option 2: Direct Access - IP Restricted (Convenient but Less Secure)**

**Security Group Configuration:**
```
Inbound Rules:
â”Œâ”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Type â”‚ Port â”‚   Source    â”‚     Description     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ SSH  â”‚  22  â”‚ Your IP/32  â”‚ SSH access          â”‚
â”‚ HTTP â”‚ 8080 â”‚ Your IP/32  â”‚ API Gateway         â”‚
â”‚ HTTP â”‚ 8761 â”‚ Your IP/32  â”‚ Eureka Dashboard    â”‚
â”‚ HTTP â”‚ 9411 â”‚ Your IP/32  â”‚ Zipkin UI           â”‚
â””â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Access:**
```bash
# Direct browser access
http://<EC2_IP>:8080  # API Gateway
http://<EC2_IP>:8761  # Eureka dashboard
http://<EC2_IP>:9411  # Zipkin UI
```

**Pros:**
- âœ… Direct browser access, no SSH tunnel needed
- âœ… Still restricted to your IP (reasonably secure)
- âœ… Convenient for frequent debugging

**Cons:**
- âš ï¸ Larger attack surface (4 ports exposed vs 2)
- âš ï¸ Must update security group when your IP changes
- âš ï¸ If your IP is compromised, attacker can see internal architecture

---

#### ğŸ¯ My Recommendation:

**Use Option 1 (SSH Tunneling)** for the best balance of security and practicality:

1. **Setup is simple**: One SSH command opens both tunnels
2. **Muscle memory**: Good habit for real production work
3. **Minimal risk**: Even if someone finds your EC2 IP, they can't access monitoring tools
4. **Learn proper practices**: Prepares you for real-world deployments

**If you prefer convenience**, use Option 2 with these precautions:
- Update security group immediately when your IP changes
- Never leave ports open when not actively working
- Monitor CloudWatch logs for unexpected access attempts

---

### 3. Data Persistence: Docker Named Volumes

Keep using Docker named volumes.
For single EC2 instance deployment, Docker named volumes are **appropriate** with important considerations:

#### âœ… Good for Your Use Case Because:
- Simple to manage - same approach for local and AWS
- Survives container restarts and `docker-compose restart`
- No external dependencies (no RDS/DocumentDB costs)
- Aligns with "practice project, few hours/month" goal

#### âš ï¸ Important Limitations:

**Data Loss Scenarios:**
```bash
docker compose down -v  # âŒ Deletes all data
docker compose down     # âœ… Preserves data
docker system prune -a --volumes  # âŒ Deletes all data
```

**EC2 Instance Replacement:**
- If EC2 terminates/fails â†’ âŒ **All data lost**
- Named volumes live on EC2's root volume (ephemeral)

#### ğŸ¯ Recommendations:

**For Now (Practice Phase):**
- âœ… Use Docker named volumes (as chosen)
- Document backup/restore procedures
- Understand data is non-persistent across instance replacements
- Note: `docker compose restart` preserves data, but `docker compose down -v` deletes it

**For Later (When You Want Durability):**

1. **Quick Win - EBS Volumes**:
   ```yaml
   volumes:
     mysql_data:
       driver: local
       driver_opts:
         type: none
         o: bind
         device: /mnt/ebs/mysql_data
   ```
   - Attach EBS volume to EC2, mount at `/mnt/ebs`
   - Survives instance replacement (detach/reattach EBS)
   - Cost: ~$10/month for 100GB

2. **Production Approach - Managed Databases**:
   - MySQL â†’ RDS (~$15-30/month for db.t3.micro)
   - MongoDB â†’ DocumentDB or Atlas ($$$ - not recommended for practice)
   - Kafka â†’ MSK ($$$)

**Decision for This Plan**: We'll use Docker named volumes as you specified, with clear documentation about data persistence characteristics.

---

### 4. Kafka Advertised Listeners

**Challenge**: Kafka needs to advertise reachable addresses. Currently configured:
```yaml
KAFKA_ADVERTISED_LISTENERS: 
  PLAINTEXT://broker:29092         # Internal (services)
  PLAINTEXT_HOST://localhost:9092  # External (local machine)
```

**For AWS EC2**:
- Internal services will connect via `broker:29092` (Docker network) âœ…
- No external Kafka clients needed âœ…
- **No changes required** to Kafka configuration

**Why No Changes:**
- All Kafka consumers/producers (inventory-service, order-service) run inside Docker network
- They use `broker:29092` via `KAFKA_BOOTSTRAP_SERVERS` environment variable
- `localhost:9092` listener is for local machine debugging (not used in AWS)

---

### 5. Eureka Service Registration: Hostname vs IP

**Observation**: 
- Single Docker host with one bridge network (`microservices-network`)
- Docker DNS provides stable hostname â†’ IP resolution
- All services reference each other by service name
- Works identically on local machine and AWS EC2

#### Why `eureka.instance.prefer-ip-address=true` is NOT needed:

**Single Host Architecture (Your Setup):**
```
EC2 Instance (or Local Machine)
    â””â”€â”€ microservices-network (Docker bridge)
        â”œâ”€â”€ discovery-server    (hostname: discovery-server)
        â”œâ”€â”€ product-service     (hostname: product-service)
        â”œâ”€â”€ inventory-service   (hostname: inventory-service)
        â””â”€â”€ order-service       (hostname: order-service)
```

**How It Works:**
1. Eureka registers `product-service` with hostname `product-service`
2. API Gateway looks up `product-service` in Eureka â†’ gets `product-service` hostname
3. API Gateway calls `http://product-service:8080/api/products`
4. Docker DNS resolves `product-service` â†’ container IP (e.g., `172.18.0.5`)
5. Request succeeds âœ…

**Why It Works on Both Local & AWS EC2:**
- Docker bridge network behavior is identical
- DNS resolution is identical
- Container naming is identical
- No hostname reachability issues because all containers share same network

#### When WOULD you need `prefer-ip-address=true`?

**âŒ NOT needed: Single Docker host (your case)**
- All containers in same Docker network
- Docker DNS handles everything perfectly
- Hostname stability guaranteed for container lifetime

**âœ… Needed: Multi-host deployments**

**Example 1: Docker Swarm (multiple EC2 instances)**
```
EC2-1: product-service (container ID: abc123, hostname: product-service.abc123)
EC2-2: inventory-service (needs to call product-service)
```
- Problem: `product-service.abc123` not resolvable from EC2-2 via Docker DNS
- Solution: Use IP addresses for cross-host communication

**Example 2: AWS ECS Fargate with awsvpc networking**
```
Each ECS task gets:
- Its own Elastic Network Interface (ENI)
- Unique private IP address
- Dynamic hostname that changes on restart
```
- Problem: Hostnames change on task restart, DNS propagation delays
- Solution: IP-based registration for faster discovery

**Example 3: Mixed deployment (Docker + non-Docker)**
```
EC2-1: Services in Docker containers
EC2-2: Services running directly on host (no Docker)
```
- Problem: Docker DNS only works inside Docker network
- Solution: IP-based registration for universal reachability

**Decision**: **NO changes needed for single EC2 deployment**

---

## AWS EC2 Setup Instructions

### 1. EC2 Instance Setup

**Instance Configuration:**
```
Instance Type:    t3.large (2 vCPU, 8GB RAM)
AMI:              Amazon Linux 2023
Storage:          20GB gp3 root volume (default value)
Security Group:   See section below
Key Pair:         Create or use existing
```

**Why t3.large?**
- Your local setup uses ~3.6GB memory
- t3.large has 8GB (leaves ~4GB buffer for OS + overhead)
- Sufficient for development/practice workload

---

### 2. Security Group Configuration

**Option 1: Direct Access (IP-Restricted)**
```
Inbound Rules:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Type   â”‚   Port   â”‚   Source    â”‚     Description     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   SSH    â”‚    22    â”‚  Your IP/32 â”‚  SSH access         â”‚
â”‚   HTTP   â”‚   8080   â”‚  Your IP/32 â”‚  API Gateway        â”‚
â”‚  Custom  â”‚   8761   â”‚  Your IP/32 â”‚  Eureka Dashboard   â”‚
â”‚  Custom  â”‚   9411   â”‚  Your IP/32 â”‚  Zipkin UI          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Outbound Rules: All traffic (default)
```

**How to get your IP:**
```bash
curl -4 ifconfig.me
# Example output: 203.0.113.42
# Use: 203.0.113.42/32 in security group
```

**Option 2: SSH Tunneling (More Secure)**
```
Inbound Rules:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Type   â”‚   Port   â”‚   Source    â”‚     Description     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   SSH    â”‚    22    â”‚  Your IP/32 â”‚  SSH access         â”‚
â”‚   HTTP   â”‚   8080   â”‚  Your IP/32 â”‚  API Gateway        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Then on local machine:
ssh -i key.pem -L 8761:localhost:8761 -L 9411:localhost:9411 ec2-user@<EC2_IP>

# Access monitoring tools via localhost
http://localhost:8761  # Eureka
http://localhost:9411  # Zipkin
```

---

### 3. EC2 User Data Script (Automated Setup)

**For Amazon Linux 2023 - Use this script in EC2 User Data during instance launch:**

```bash
#!/bin/bash
set -e

# Log output to file for debugging
exec > >(tee /var/log/user-data.log)
exec 2>&1

echo "Starting OrderProducts setup on Amazon Linux 2023..."

# Update system (dnf is the package manager for AL2023)
dnf update -y

# Install Docker
dnf install -y docker
systemctl start docker
systemctl enable docker

# Add ec2-user to docker group
usermod -aG docker ec2-user

# Install Docker Compose V2 as Docker CLI plugin (modern method for AL2023)
DOCKER_COMPOSE_VERSION="v2.24.5"
mkdir -p /usr/local/lib/docker/cli-plugins
curl -SL "https://github.com/docker/compose/releases/download/${DOCKER_COMPOSE_VERSION}/docker-compose-linux-$(uname -m)" \
  -o /usr/local/lib/docker/cli-plugins/docker-compose
chmod +x /usr/local/lib/docker/cli-plugins/docker-compose

# Create symlink for backward compatibility (optional)
ln -sf /usr/local/lib/docker/cli-plugins/docker-compose /usr/local/bin/docker-compose

# Install Git
dnf install -y git

# Clone repository (replace with your GitHub username)
cd /home/ec2-user
git clone https://github.com/YOUR_USERNAME/OrderProducts.git
chown -R ec2-user:ec2-user OrderProducts

# Pull Docker images (as ec2-user)
su - ec2-user -c "cd /home/ec2-user/OrderProducts/infrastructure && docker compose pull"

# Start services (as ec2-user)
su - ec2-user -c "cd /home/ec2-user/OrderProducts/infrastructure && docker compose up -d"

# Setup log rotation for Docker containers
cat > /etc/logrotate.d/docker-containers <<'EOF'
/var/lib/docker/containers/*/*.log {
    rotate 7
    daily
    compress
    missingok
    delaycompress
    copytruncate
}
EOF

# Wait for services to be healthy
echo "Waiting for services to start (this may take 3-5 minutes)..."
sleep 180

# Check service status
su - ec2-user -c "cd /home/ec2-user/OrderProducts/infrastructure && docker compose ps"

echo "OrderProducts setup complete!"
echo "Access API Gateway at: http://$(curl -s http://169.254.169.254/latest/meta-data/public-ipv4):8080"
```

**Important Notes:**
- Replace `YOUR_USERNAME` with your actual GitHub username
- This script uses `dnf` (Amazon Linux 2023's package manager)
- Docker Compose V2 is installed as a plugin (uses `docker compose` with space, not `docker-compose` with hyphen)
- Script logs are saved to `/var/log/user-data.log` for debugging

---

### 4. Manual Setup (Alternative to User Data)

If you prefer manual setup on **Amazon Linux 2023**:

**Step 1: Connect to EC2**
```bash
ssh -i your-key.pem ec2-user@<EC2_PUBLIC_IP>
```

**Step 2: Install Docker**
```bash
# Update system
sudo dnf update -y

# Install Docker
sudo dnf install -y docker

# Start and enable Docker
sudo systemctl start docker
sudo systemctl enable docker

# Add ec2-user to docker group
sudo usermod -aG docker ec2-user

# Verify Docker installation
docker --version
```

**Step 3: Install Docker Compose V2 (as plugin)**
```bash
# Create plugin directory
sudo mkdir -p /usr/local/lib/docker/cli-plugins

# Download Docker Compose V2
DOCKER_COMPOSE_VERSION="v2.24.5"
sudo curl -SL "https://github.com/docker/compose/releases/download/${DOCKER_COMPOSE_VERSION}/docker-compose-linux-$(uname -m)" \
  -o /usr/local/lib/docker/cli-plugins/docker-compose

# Make executable
sudo chmod +x /usr/local/lib/docker/cli-plugins/docker-compose

# Create symlink (optional, for backward compatibility)
sudo ln -sf /usr/local/lib/docker/cli-plugins/docker-compose /usr/local/bin/docker-compose

# Verify installation
docker compose version
```

**Step 4: Start Services**

First time (or after instance start)

```bash
docker login -u ashishjha
docker compose pull
docker compose up -d
```

Wait ~1â€“2 minutes.

Verify:

```bash
docker ps
docker stats
```

**Step 5: Verify Deployment**
```bash
# From EC2, test services locally
curl http://localhost:8761/eureka/apps  # Eureka
curl http://localhost:8080/actuator/health  # API Gateway

# From your local machine, test external access
curl http://<EC2_PUBLIC_IP>:8080/actuator/health
```

---

## Maintenance & Operations

### Backup & Restore

**Backup (Manual):**
```bash
# SSH to EC2
ssh -i key.pem ec2-user@<EC2_IP>

# Create backup directory
mkdir -p /home/ec2-user/backups

# Backup MySQL data
sudo docker run --rm \
  -v mysql_data:/data \
  -v /home/ec2-user/backups:/backup \
  ubuntu tar czf /backup/mysql_data_$(date +%Y%m%d_%H%M%S).tar.gz -C /data .

# Backup MongoDB data
sudo docker run --rm \
  -v mongodb_data:/data \
  -v /home/ec2-user/backups:/backup \
  ubuntu tar czf /backup/mongodb_data_$(date +%Y%m%d_%H%M%S).tar.gz -C /data .

# Backup Kafka data
sudo docker run --rm \
  -v kafka_data:/data \
  -v /home/ec2-user/backups:/backup \
  ubuntu tar czf /backup/kafka_data_$(date +%Y%m%d_%H%M%S).tar.gz -C /data .

# Download backups to local machine (from local terminal)
scp -i key.pem ec2-user@<EC2_IP>:/home/ec2-user/backups/*.tar.gz ./
```

**Restore:**
```bash
# SSH to EC2
ssh -i key.pem ec2-user@<EC2_IP>

# Stop services
cd /home/ec2-user/OrderProducts/infrastructure
docker compose down

# Restore MySQL data
sudo docker run --rm \
  -v mysql_data:/data \
  -v /home/ec2-user/backups:/backup \
  ubuntu tar xzf /backup/mysql_data_20231228_120000.tar.gz -C /data

# Restore MongoDB data
sudo docker run --rm \
  -v mongodb_data:/data \
  -v /home/ec2-user/backups:/backup \
  ubuntu tar xzf /backup/mongodb_data_20231228_120000.tar.gz -C /data

# Start services
docker compose up -d
```

**Automated Backup (Cron):**
```bash
# Create backup script
cat > /home/ec2-user/backup_volumes.sh <<'EOF'
#!/bin/bash
BACKUP_DIR="/home/ec2-user/backups"
DATE=$(date +%Y%m%d_%H%M%S)

mkdir -p ${BACKUP_DIR}

# Backup MySQL
sudo docker run --rm \
  -v mysql_data:/data \
  -v ${BACKUP_DIR}:/backup \
  ubuntu tar czf /backup/mysql_${DATE}.tar.gz -C /data .

# Backup MongoDB
sudo docker run --rm \
  -v mongodb_data:/data \
  -v ${BACKUP_DIR}:/backup \
  ubuntu tar czf /backup/mongodb_${DATE}.tar.gz -C /data .

# Keep only last 7 days
find ${BACKUP_DIR} -name "*.tar.gz" -mtime +7 -delete

echo "Backup completed: ${DATE}"
EOF

chmod +x /home/ec2-user/backup_volumes.sh

# Add to crontab (daily at 2 AM)
(crontab -l 2>/dev/null; echo "0 2 * * * /home/ec2-user/backup_volumes.sh >> /home/ec2-user/backup.log 2>&1") | crontab -
```

---

### Monitoring

**Basic Monitoring (Free):**
```bash
# Check service status
docker compose ps

# Check resource usage
docker stats --no-stream

# Check container health
docker inspect --format='{{.State.Health.Status}}' api-gateway

# Check logs
docker compose logs --tail=100 -f
```

**CloudWatch Monitoring (Optional):**

Install CloudWatch agent:
```bash
# Download CloudWatch agent
wget https://s3.amazonaws.com/amazoncloudwatch-agent/amazon_linux/amd64/latest/amazon-cloudwatch-agent.rpm

# Install
sudo rpm -U ./amazon-cloudwatch-agent.rpm

# Configure (requires IAM role with CloudWatch permissions)
sudo /opt/aws/amazon-cloudwatch-agent/bin/amazon-cloudwatch-agent-config-wizard
```

**Metrics to monitor:**
- CPU utilization (alert if > 80%)
- Memory utilization (alert if > 85%)
- Disk utilization (alert if > 80%)
- Network in/out
